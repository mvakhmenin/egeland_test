# Задание 3: Кодирование, Python и SQL оптимизация
## У вас есть данные о заказах пользователей. Напишите Python-скрипт, который читает данную таблицу (файл) с данными о заказах. Ваша задача — загрузить их в БД, провести оптимизированную обработку и написать SQL-запросы. Формат: Тестовая таблица
## Часть 1: Оптимизированная загрузка в БД
### Напишите ETL-скрипт, который:
- Читает CSV-файл с заказами (1 млн строк).
- Выполняет нормализацию данных (выносит курсы и предметы в справочные таблицы).
- Использует batch-загрузку (COPY вместо INSERT).
- Работает асинхронно (например, asyncpg для PostgreSQL).

Код ETL-скрипта реализован в файле `etl.py`. Для запуска скрипта необходимо установить зависимости из файла `requirements.txt`. В качестве параметра необходимо передать в скрипт имя CSV-файла с данными для загрузки. Скрипт создает пустые таблицы для нормализованной структуры данных (таблицы-справочники и основная таблица с данными), номрмализует данные, записывает нормализованные данные в таблицы-справочники и основную таблицу. Запись в таблицы-справочники производится через `INSERT` (так как объем данных в справочниках относительно небольшой), запись в основную таблицу производится с помощью batch-загрузки через `COPY`. Скрипт работает асинхронно.

### Как ускорить загрузку данных?
- Какие параметры БД (work_mem, shared_buffers) можно настроить?

work_mem - Определяет память для операций сортировки, хеш-соединений и агрегации в рамках одной операции (Увеличивает скорость HashJoin и HashAggregate). shared_buffers - Определяет размер кеша PostgreSQL для всех данных и индексов в памяти (Уменьшает количество физических чтений с диска)

Лучший результат даст сочетание:
1. Адекватный размер shared_buffers (кеш данных)
2. Достаточный work_mem (оперативная память для обработки)
3. Правильные индексы (уменьшение объёма обрабатываемых данных)

- Как избежать блокировок таблиц при массовой вставке?

Для ускорения загрузки были предприняты следующие действия:
1.  Внешние ключи основной таблицы были добавлены только после окончания загрузки данных.
2. Для записи основного массива данных используется COPY вместо INSERT (минимальные блокировки).
3. Объем данных для вставки разбит на батчи (в коде определен размер 1000 строк для тестовых данных, на реальных данных, возможно, стоит повысить до 10-20 тыс. строк).

## Часть 2: Оптимизация SQL-запросов
1. Напишите SQL-запрос, который возвращает ТОП-5 самых продаваемых курсов по месяцам.
2. Напишите SQL-запрос, который возвращает ТОП-3 самых популярных пакетов по предметам.

Текст запросов см. в файле `requests.sql`

### Оптимизируйте запросы:
- Какие индексы добавить?

Для оптимизации запроса множно добавить индексы по полям фильтрации, соединения, фильтрации и сортировки. В таблицах справочниках сразу созданы `PRIMARY KEY`, которые являются индексами. Поэтому индексы нужно добавлять по основной таблице `orders`. Для оптимизируемых запросов можно добавить индексы по полям course_id, subject_id, package_id (PostgreSQL не создает автоматически индекс на внешние ключи). Так как группировка производится по нескольким полям, необходимо создавать составные индексы (по полям группировки).

1. Необходимо создать составной индекс по полям DATE_TRUNC('MONTH', order_ts) и course_id

`CREATE INDEX idx_orders_month_course_id ON orders(DATE_TRUNC('MONTH', order_ts), course_id)`

В результате получаем Index Scan основной таблицы `orders` (на тестовых данных Seq Scan из-за малого количества данных). Однако на дальнейших этапах значительно ускоряется сортировка и индекс показывает свою эффективность.

2. Необходимо создать составной индекс по полям subject_id и package_id

`CREATE INDEX idx_orders_subject_id_package_id ON orders(subject_id, package_id)`

В результате по плану запроса получаем Seq Scan только для маленьких таблиц-справочников subjects и packages. По основной таблиц orders получаем Index Only Scan (то есть обращение к самой таблице вообще не происходит - только к индексу). Итоговое снижение cost на тестовых данных получилось небольшим (около 5%) за счет того, что основное время пришлось на финальную сортировку результата (при этом без учета финальной сортировки cost снизились примерно в 10 раз). На реальных данных количество строк в таблице orders возрастет на порядки, а количество категорий группировки значительно меньше (максимум в разы), поэтому увеличение времени финальной сортировки возрастет не значительно. Кроме того, можно попробовать переложить работу по финальной сортировки на конечного клиента (например, BI-систему), что снизит нагрузку на СУБД.

### Какую стратегию партиционирования выбрать (range, hash)?

Эффективной будет стратегия партиционирования RANGE (по месяцам). 

Преимущества:
* Исключение полного сканирования таблицы при запросе по конкретному месяцу
* Возможность быстрой очистки старых данных через DROP PARTITION
* Параллельное сканирование разных партиций

Несмотря на то что во втором запросе нет фильтрации / агрегации по дате, запрос будет более эффективным за счет параллельного сканирования.


### Как минимизировать full table scan?
1. Партиционирование - возможность исключения партиций из запроса.
2. Добавление индексов для таблицы: частичное сканирование при фильтрации и INDEX ONLY SCAN, если все поля запроса входят в индекс.
3. Создание материализованных представлений для исторических данных (которые не будут меняться в будущем) и предварительно расчитанных агрегированных витрин.