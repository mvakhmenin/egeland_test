# Задание 1: Интеграция с внешними API (TikTok, Instagram) с учетом масштабируемости

## 1. Реализовать интеграцию с API TikTok и Instagram:
### Собрать данные о лайках, просмотрах, репостах для списка пользователей.
Код парсера TikTok реализован в файле `tiktok_parser.py`. В связи с тем что получение токена авторизации для TikTok API занимает много времени парсинг данных осуществляется через HTTP-запросы (requests) с дальнейшей очисткой полученных данных. Для запуска парсинга необходимо установить зависимости (`requirements.txt`) и развернуть хранилище данных (см. п.2 настоящего задания)

### Оптимизировать работу с API (использование параллельных запросов, кеширование токенов аутентификации).
Вместо использования параллельных запросов предлагается при необходимости ускорения парсинга путем разделения на несколько потоков запускать параллельно несколько Docker-контейнеров, каждый из который будет ответчать за парсинг своего диапазона пользователей. Для этого скрипт парсера поддерживает параметры LIMIT и OFFSET (значение параметров аналогичны их значеням в SQL). Алгоритм запуска может быть следующий: получение общего количества пользователей для парсинга из БД -> определение количества необходимых Docker-контейнеров исходя из заранее заданного размера батча пользователей на контейнер -> запуск необходимого количестав Docker-контейнеров с передаче в ENTRYPOINT соответствующих значений LIMIT и OFFSET.

Преимущества использования нескольких параллельных Docker-контейнеров перед параллельными запросами:
* Горизонтальное масштабирование: Можно легко увеличить скорость парсинга путем уменьшения размера батча (и соотвественно увеличения количестав параллельных Docker-контейнеров) без переписывания кода.
* Устойчивость к блокировкам и банам: Каждый контейнер может использовать свой прокси/VPN, снижая риск блокировки.
* Изоляция падений: Если один контейнер упадет, остальные продолжат работу. Упашвий контейнер легко перезапускается без влияния на результаты других.
* Логирование и мониторинг: Логи каждого контейнера изолированы.

### Обрабатывать ограничения API (rate limits, ошибки 429, 503).
В связи с ограничением по времени разработки ошибки обрабатываются простым способом - через задержку выполнения кода. Время ожидания параметризовано в коде. Можно использовать более сложные алгоритмы, например, экспоненциальную задержку.


## 2. Спроектировать хранилище данных:
### Как структурировать таблицы в PostgreSQL / BigQuery?
Структура данных хранилища изложена в файле `create_tables.sql`. Основная таблица для хранения метрик `user_stats`, она соединена по внешним ключам с таблицам `platforms` и `users` для контроля целостности данных. Каждая метрика хранится в отдельном столбце. Потенциально это может привести к проблемам при необходимости сохранять данные по другим метрикам (но в задании информации об этом не было).

Для того чтобы сделать схему данных более гибкой можно выделить каждую метрику в отдельную таблицу. В результате добавить новую метрику будет очень просто - путем создания новой таблицы. Из минусов: большое количество JOIN для сбора всей статистики по пользователю и усложнение схемы данных. 

Другой вариант - хранить метрики в формате JSON. Тогда можно добавлять любой набор метрик. Из минусов: остуствие контроля целостности данных в JSON-столбце и более низкася скорость работы с JSON-столбцом, чем с обычными столбцами.

Для расширения перечня хранимых метрик предлагается использовать дополнителью таблицу `user_stats_extended`, которая соединяется с таблице `user_stats` по `id`(`stats_id`), куда метрики запиываются в формате ключ-значение. Такая таблица может быть удобной для временного сбора метрик (например, для оценки эффективности рекламной компании, которая ограничена во времени).

### Как избежать дублирования данных?
Контроль за дубликатами осуществляется путем добавления уникального ключа по полям user_id, platform_id, record_hour таблицы user_stats. В случае вставки дубликата в таблицу данные метрик обновляются через механизм `INSERT ... ON CONFLICT ON CONSTRAINT ... DO UPDATE`

Дополнительно можно настроить процедуру проверки таблицы на дубликаты во время минимальной загрузки сервиса (например, ночью).

### Как спроектировать партиционирование и индексацию?
Для корректного партиционирования и индексации таблиц необходимо понимать структуру данных и какие запросы будут отправляться на выборку данных.
В текущем решении предложено партиционировать таблицу `user_stats` по `user_id`, так как предполагается, что будет выгружаться статистика по конкретному пользователю. `user_id` низкокардинальный столбец и стандартный b-tree индекс будет по нему не эффективным. Партиционирование позволит использовать только одну партицию по пользователю при выборке данных вместо полного сканирования всей таблицы. Однако партиционирование по `user_id` можно использовать только если количество пользователей не более 2-3 тысяч для PostgreSQL (для BigQuery побольше, но все равно количество партиций ограничено). Для автоматизированного создания партиций в файле `create_tables.sql` реализован код процедуры `create_user_partition()` и триггера `trg_create_user_partition`.

Так как в таблицу `user_stats` будут постоянно вставляться данные, то избыточное количество индексов приведет к значительному росту нагрузки на СУБД в связи с перестройкой индексов при вставке. Таблицы `platforms` и `users` очень маленькие (максимум десятки и тысячи строк соответственно), поэтому никакие специальные индексы для них не нужны (для таблицы `platforms` индексы скорее всего использоваться не будут оптимизатором запросов, так как накладные расходы на использование индекса не покроют возмможную экономию).

Так как таблица `user_stats` будет соединяться с таблицами `platforms` и `users` индексы по ключам соединения могут помочь ускорить JOIN. Для таблицы `user_stats` создан уникальный индекс по полям `user_id`, `platform_id`, `record_hour`, поэтому запросы по `user_id` будут использовать этот индекс (к `user_id` можно добавить еще `platform_id` и `record_hour` - именно в такой комбинации. Запрос только по `platform_id` не будет использовать индекс). Поле `platform_id` очень низкокардинальное (скорее всего единицы уникальных значений), поэтому индекс для него будет не эффективным. Таким образом, дополнительно достаточно создать только индекс по `record_hour` для сортировки (timeseries скорее всего будут сортироваться)

Дополнительно хочу отметить, что описанная выше нагрузка на СУБД является классической OLAP нагрузкой (фиксируется большое количество событий-фактов, которые не подлежать изменению, основные операции с этими данными - выборка и агрегация) и выбор OLTP СУБД PostgreSQL нельзя назвать оптимальным. Выбор OLAP СУБД со столбцовым хранением такой как Clickhouse может значительно повысить эффективность. Для использования Clickhouse необходимо внести следующие изменения в представленное решение:
* Данные парсинга должны хранится в широкой денормализованой таблице. Отсутствие необходимости в JOIN и столбцовое хранение упрощают схему данных (метрики хранятся в столбцах широкой таблицы), денормализованные столбцы сжимаются движком СУБД для экономии места.
* Запись должна осуществляться крупными батчами, либо с использованием промежуточных таблиц-накопителей (например, с движком Kafka)

## 3. Реализовать обработку в реальном времени:
### Какие инструменты потоковой обработки (Kafka, Spark Streaming, Flink) и как использовать?
Непонятен сам смысл использования потоковой обработки для данных, которые поступают раз в час. Сами данные меняются не настолько часто, чтобы использовать потоковую обработку. Также непонятно, зачем реагировать на изменения данных немедленно (в этом смысл потоковой обработки), что произойдет, если сервис среагирует на изменение метрик с задержкой в несколько часов?

Тем неменее, можно перестроить ETL-пайплайн, заменив на последней стадии функцию записи в БД stats_2_db на запись в топик Kafka. Из топика Kafka данные можно забирать, например, в Clickhouse через таблицу с движком Kafka.

### Как реализовать мониторинг ETL? (например, Prometheus + Grafana)
Для мониторинга записываем в Prometheus (через push_to_gateway) время получения данных по каждому пользователю. Потом в Grafana можно построить дашборд по количеству полученных данных, среднему времени получения данных по пользователю, общему времени выполнения парсинга, количеству запущенных контейнеров.

## 4. Оценить производительность:
### Как обрабатывать большой объем записей в сутки? Как минимизировать нагрузку на БД?
В случае использования Kafka: создать топик для записи результатов парсинга с партициями по максимальному числу запускаемых контейнеров, на каждом шарде кластера создать таблицу с движком Kafka для параллельного чтения из партиций топика (сумма параметров kafka_num_consumers для всех таблиц должна соответствовать количеству партиций), создать соответствующие материализованные представления и целевые таблицы на каждом шарде для переноса и сохранения данных из таблицы с движком Kafka, создать Distirbuted таблицу для параллельного чтения данных Kafka из целевых таблиц на каждом шарде. В результате получаем максиальную скорость записи (масштабируется увеличением числа запускаемых котейнеров и партиций топика) и максимальную скорость чтения (масштабируется увеличением числа шародов в кластере Clickhouse и kafka_num_consumers в параметрах таблиц)

Если все-таки нужно будет записывать большой объем данных в PostgreSQL, то самым быстрым вариантом будет сохранить результаты работы всех контейнеров-парсеров в файлы и загрузить их в целевую таблицу через механизм `COPY` вместо `INSERT`. Детально архитектура данного ETL-пайплайна и доступные механизмы повышения скорости записи рассмотрены в `Задании 3`